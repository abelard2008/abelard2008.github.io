#+STARTUP: showall indent
#+STARTUP: hidestars
#+OPTIONS:   H:2 num:nil toc:nil \n:nil ::t |:t -:t f:t *:t <:t

#+OPTIONS:   tex:t  d:nil todo:t pri:nil tags:not-in-toc
#+BEGIN_HTML
---
category: computer science and enginneering
layout: post
title: "Notes of HOP"
Tags: ["Big Data", Hadoop, HOP]
---
#+END_HTML

* HOP 对hadoop修改的内容

- 添加了org.apache.hadoop.mapred.buffer，包括OutputFile(类)，Manager(类),BufferUmbilicalProtocol(接口),
- 添加了org.apache.hadoop.mapred.buffer.impl，包括了JOutputBuffer
#+BEGIN_SRC java
	public class JOutputBuffer<K extends Object, V extends Object> 
		extends Buffer<K, V>
			implements OutputCollector<K, V>, IndexedSortable {
		}
#+END_SRC
在MapTask和ReduceTask中，添加了根据应用程序是否设置pipeline，来调用实例化JOutputBuffer，进而使用push或原来的pull方法，在map和reduce之间传递数据。

在运行TaskRunner的run方法时，org.apache.hadoop.mapred.child通过使用下面的方法进行了注册
#+BEGIN_SRC java
	// Add main class and its arguments 
		vargs.add(Childx.class.getName());  // main of Child
#+END_SRC
在TaskTrack的run方法中调用 offerService()方法， offerService()调用addToTaskQueue((LaunchTaskAction)action)方法。

在addToTaskQueue方法中调用registerTask方法
#+BEGIN_SRC java
	public void addToTaskQueue(LaunchTaskAction action) {
		TaskInProgress tip = registerTask(action, this);
	}
#+END_SRC
在registerTask方法中创建了TaskInProgress实例
#+BEGIN_SRC java
	private TaskInProgress registerTask(LaunchTaskAction action, 
		TaskLauncher launcher) {
			Task t = action.getTask();
				TaskInProgress tip = new TaskInProgress(t, this.fConf, launcher);
			}
#+END_SRC
通过
#+BEGIN_SRC java
	Task t = action.getTask();
#+END_SRC
可以看出具体是什么类型的task。

/org.apache.hadoop.ipc.Client$Connection/ 的run方法调用receiveResponse()方法,receiveResponse()调用 /org.apache.hadoop.io.ObjectWritable/ 的readFields(in)方法,进而调用readObject()方法,readObject()调用 /org.apache.hadoop.mapred.HeartbeatResponse的readFields()/
#+BEGIN_SRC java
	public void readFields(DataInput in) throws IOException {
		this.responseId = in.readShort();
			this.heartbeatInterval = in.readInt();
				int length = WritableUtils.readVInt(in);
				if (length > 0) {
					actions = new TaskTrackerAction[length];
						for (int i=0; i < length; ++i) {
							TaskTrackerAction.ActionType actionType = 
								WritableUtils.readEnum(in, TaskTrackerAction.ActionType.class);
									actions[i] = TaskTrackerAction.createAction(actionType);
								actions[i].readFields(in);
							}
						...
					}
#+END_SRC
在这个方法中，创建了TaskTrackerAction实例actions，根据ActionType的类型调用相应的动作，以LAUNCH_TASK为例，它就会调用 /org.apache.hadoop.mapred.LaunchTaskAction/ 的readFields()方法。
#+BEGIN_SRC java
	public void readFields(DataInput in) throws IOException {
	  
		  boolean isMapTask = in.readBoolean();
		  boolean isPipeline = in.readBoolean();
			  if (isMapTask) {
				  task = isPipeline ? new PipelineMapTask() : new MapTask();
			  } else {
				  task = new ReduceTask();
			  }
				  task.readFields(in);
			  }
		  }
#+END_SRC
在这个方法中，就确定了是要启动MapTask还是ReduceTask

在TaskInProgress的task成员是如何与TaskRunner联系起来的，在有mapreduce任务时，怎么触发TaskRunner中的run方法?

根据上面的解释，知道task是MapTask或ReduceTask，因此在运行到TaskTracker的launchTask时，task.createRunner()就会去执行，MapTask中的createRunner()方法: (在TaskTracker中，launchTaskForJob->launchTask->this.runner = task.createRunner(TaskTracker.this, this);)

#+BEGIN_SRC java
	@Override
		public TaskRunner createRunner(TaskTracker tracker, TaskTracker.TaskInProgress tip) {
			return new MapTaskRunner(tip, tracker, this.conf);
		}
#+END_SRC
从类的关系知道，MapTaskRunner继承自TaskRunner，因此当task需要运行run方法时，就去执行TaskRunner的run方法。

在 hadoop-hop-0.2/logs/hadoop-abelard-tasktracker-dillon.cloud.log开始的那些行

#+BEGIN_SRC java
2012-02-11 19:36:37,110 INFO org.apache.hadoop.mapred.TaskTracker: STARTUP_MSG: 

/************************************************************/

STARTUP_MSG: 

STARTUP_MSG:   host = dillon.cloud/192.168.0.88

STARTUP_MSG:   args = []

STARTUP_MSG:   version = 0.2

STARTUP_MSG:   build =  -r ; compiled by 'abelard' on Sat Feb 11 19:35:32 CST 2012
#+END_SRC
出自：org.apache.hadoop.util.StringUtils的startupShutdownMessage()方法。因此分别在JobTrack和TaskTrack的main()方法中，见到对startupShutdownMessage()方法的调用。


